{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Analysis_Optimised_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TITHI007/Tweet_Extraction_Analysis_Internship/blob/main/Analysis_Optimised_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06992675",
        "outputId": "648768e5-d9b9-468d-8b50-db92036a94c0"
      },
      "source": [
        "pip install setuptools"
      ],
      "id": "06992675",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a0f59bf",
        "outputId": "710fa7cd-b7de-4bda-c25b-660ae7c89aec"
      },
      "source": [
        "pip install germansentiment"
      ],
      "id": "3a0f59bf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting germansentiment\n",
            "  Downloading germansentiment-1.0.6-py3-none-any.whl (4.9 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from germansentiment) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->germansentiment) (3.10.0.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (1.19.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (3.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (21.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->germansentiment) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 17.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->germansentiment) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->germansentiment) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->germansentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->germansentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->germansentiment) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->germansentiment) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->germansentiment) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->germansentiment) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->germansentiment) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, germansentiment\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed germansentiment-1.0.6 huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "824f5649",
        "outputId": "3a08ab85-28b8-4395-bb90-0c5eec894e96"
      },
      "source": [
        "pip install utils"
      ],
      "id": "824f5649",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e266e3f",
        "outputId": "75aa8b8e-b901-456a-ee55-15fdc8bd6d56"
      },
      "source": [
        "pip install transformers"
      ],
      "id": "2e266e3f",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "289d5dd4"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from typing import List\n",
        "import torch\n",
        "import re\n",
        "from germansentiment import SentimentModel\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "import pandas as pd\n",
        "from glob import glob"
      ],
      "id": "289d5dd4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fdb1b3b"
      },
      "source": [
        "def twitterDataset(path):\n",
        "    \"\"\"\n",
        "    This function is to divide data into the main tweets and reply tweets.\n",
        "    main : a list of main tweets\n",
        "    replies: a list of reply tweets\n",
        "\n",
        "    Key list\n",
        "    Main keys:\n",
        "    ['author_id', 'conversation_id', 'created_at', 'id', 'lang', 'possibly_sensitive', 'public_metrics',\n",
        "    'text', 'replies', 'source']\n",
        "    We use 'lang', 'text', and 'replies'.\n",
        "\n",
        "    Reply keys:\n",
        "    ['author_id', 'conversation_id', 'created_at', 'id', 'lang', 'possibly_sensitive', 'public_metrics',\n",
        "    'in_reply_to_status_id', 'referenced_tweets', 'reply_settings', 'source', 'text', 'replies']\n",
        "    In case that there is no any reply, 'replies' key is not generated.\n",
        "    \"\"\"\n",
        "    n=0\n",
        "    with open(path) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        Main_Tweets=[]\n",
        "#         Replies=[]\n",
        "        for i in data:\n",
        "            Main_Tweets.append(i['text'])\n",
        "            replies=i['replies']\n",
        "            for j in replies:\n",
        "                reply_levels(j,n)\n",
        "                \n",
        "    return Main_Tweets, Replies"
      ],
      "id": "0fdb1b3b",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf315400"
      },
      "source": [
        "Replies=[]       \n",
        "def reply_levels(j,n):\n",
        "    \n",
        "    if len(Replies)<=n:\n",
        "        Replies.append([])\n",
        "    Replies[n].append(j['text'])\n",
        "   \n",
        "    \n",
        "    if j['public_metrics']['reply_count']!=0:\n",
        "        rep=j['replies']\n",
        "        n=n+1  \n",
        "        # print(n)\n",
        "        #replies'{}'.format(n)=j['replies']\n",
        "        for r in rep:\n",
        "            j=r\n",
        "            reply_levels(j,n)\n",
        "        \n",
        "    return\n",
        "    "
      ],
      "id": "bf315400",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b79bc3dd"
      },
      "source": [
        "def category_to_label(data):\n",
        "    \"\"\"\n",
        "    This function is to convert categorical string labels into numerical labels.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    count_neutral = 0\n",
        "    count_neg = 0\n",
        "    count_pos = 0\n",
        "    for d in data:\n",
        "        if d == 'neutral':\n",
        "            labels.append(1)\n",
        "            count_neutral += 1\n",
        "        elif d == 'negative':\n",
        "            labels.append(0)\n",
        "            count_neg += 1\n",
        "        elif d == 'positive':\n",
        "            labels.append(2)\n",
        "            count_pos += 1\n",
        "        else:\n",
        "            print(\"Something wrong: \", d)\n",
        "\n",
        "    print(f\"Positive: {count_pos}, Neutral: {count_neutral}, Negative: {count_neg} tweets\")\n",
        "    return labels, count_pos, count_neutral, count_neg\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # Test dataloader\n",
        "#     \"\"\"\n",
        "#     As a test code, this code below is not related to our analysis code.\n",
        "#     \"\"\"\n",
        "#     main, replies= twitterDataset('ethikrat.json')\n",
        "#     print(len(main))\n",
        "#     print(len(replies))\n",
        "#     print(replies)\n",
        "#     # print(count)\n",
        "\n"
      ],
      "id": "b79bc3dd",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8e37be",
        "outputId": "bc9d3343-a0e3-4fbd-c084-10c573701430"
      },
      "source": [
        "\n",
        "def average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    get_datafile = glob('./*.json')\n",
        "    user_data = []\n",
        "    for data in get_datafile:\n",
        "        if data.split('_')[-1] != 'Tweets.json':\n",
        "            user_data.append(data)\n",
        "    print(user_data)\n",
        "    # Load Bert Model\n",
        "    \n",
        "    model = SentimentModel()\n",
        "\n",
        "    for filepath in user_data:\n",
        "            print(filepath)\n",
        "        # try:\n",
        "            user = filepath.split('/')[-1].split('.')[0]\n",
        "            print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "            print(\"User:\", user)\n",
        "            Replies=[]\n",
        "\n",
        "            # Load Data\n",
        "            \"\"\"\n",
        "            This function is to rearrange main and reply tweets (on the first level) from the meta dataset.\n",
        "            \"\"\"\n",
        "            main, replies= twitterDataset(filepath)\n",
        "\n",
        "            \n",
        "            main_pred = model.predict_sentiment(main)\n",
        "            print(\"Prediction on the main tweets\")\n",
        "            main_pred, count_pos_main, count_neutral_main, count_neg_main = category_to_label(main_pred)\n",
        "            print(f\"Main User Tendency: {average(main_pred)}\")\n",
        "            res = [average(main_pred), count_pos_main, count_neutral_main, count_neg_main]\n",
        "            index=['AvgMain','PosMain','NeuMain','NegMain']\n",
        "            \n",
        "            for a in range(len(replies)):\n",
        "                batch_size = 3000\n",
        "                n = int(len(replies[a])/batch_size)\n",
        "                if len(replies[a]) < batch_size:\n",
        "                    reply_pred = model.predict_sentiment(replies[a])\n",
        "                else:\n",
        "                    reply_pred = []\n",
        "                    for i in range(n):\n",
        "                        if i < n-1:\n",
        "                            reply_pred += model.predict_sentiment(replies[a][i*batch_size:(i+1)*batch_size])\n",
        "                        else:\n",
        "                            reply_pred += model.predict_sentiment(replies[a][i*batch_size:])\n",
        "                # reply_pred=model.predict_sentiment(replies[a])\n",
        "                reply_pred, count_pos_reply, count_neutral_reply, count_neg_reply = category_to_label(reply_pred)\n",
        "                res.extend([average(reply_pred), count_pos_reply, count_neutral_reply, count_neg_reply])\n",
        "                index.extend(['AvgRep{}'.format(a+1),'PosRep{}'.format(a+1),'NeuRep{}'.format(a+1),'NegRep{}'.format(a+1)])\n",
        "                print('level{}'.format(a+1))\n",
        "           \n",
        "            df = pd.DataFrame(res)\n",
        "            df = pd.DataFrame(res, index, columns=[\"Analysis\",user])\n",
        "            df.to_csv(user+'.csv') #, index=False\n",
        "\n",
        "        # except:\n",
        "        #     print(\"NA\")"
      ],
      "id": "cc8e37be",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./alain_berset.json']\n",
            "./alain_berset.json\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "User: alain_berset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfCS9prxjybN"
      },
      "source": [
        ""
      ],
      "id": "hfCS9prxjybN",
      "execution_count": null,
      "outputs": []
    }
  ]
}